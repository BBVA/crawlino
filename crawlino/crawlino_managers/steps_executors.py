import logging

from crawlino.current_config import current_config
from crawlino.concurrency import map_concurrently, execute

from crawlino.models.plugins_models import PluginReturnedData

log = logging.getLogger("crawlino")


# --------------------------------------------------------------------------
# Config plugins
# --------------------------------------------------------------------------
def execute_config_step(crawler_name: str):

    crawler = current_config.get_config_crawler(crawler_name)

    for model in crawler.model.config:
        execute(model.module_name,
                model.type,
                None,
                model.config)


# --------------------------------------------------------------------------
# Sources step always runs synchronous and in serial mode
# --------------------------------------------------------------------------
def execute_sources_step(crawler_name: str):
    crawler = current_config.get_config_crawler(crawler_name)

    c = None
    for model in crawler.model.sources:

        # Generated by $generators
        c = map_concurrently(crawler_name=crawler_name,
                             previous_result=None,
                             model=model,
                             callback="execute_input_step")

    return c


# --------------------------------------------------------------------------
# Input module
# --------------------------------------------------------------------------
def execute_input_step(previous_data: PluginReturnedData,
                       crawler_name: str):
    crawler = current_config.get_config_crawler(crawler_name)

    for model in crawler.model.input:
        map_concurrently(crawler_name=crawler_name,
                         previous_result=previous_data,
                         model=model,
                         callback="execute_extractors_step")


def execute_extractors_step(previous_data: PluginReturnedData,
                            crawler_name: str):
    """Extractors runs in serial mode"""
    crawler = current_config.get_config_crawler(crawler_name)

    result = []
    for i, rule_set in enumerate(crawler.model.extractors):
        # ---------------------------------------------------------------------
        # RuleSet space
        # ---------------------------------------------------------------------
        exit_on_found = rule_set.exit_on_match
        report = rule_set.report

        #
        # - First execution content are the previous results from back step
        # - From second execution, value content come from plugin execution
        #
        if i == 0:
            content = previous_data.json_property(rule_set.input_var)
        else:
            content = r["content"]

        total_rules = len(rule_set.rules) - 1
        for rule_number, rule in enumerate(rule_set):

            # Attach input var to the config
            rule.config["content"] = content

            r = execute(rule.module_name,
                        rule.type,
                        content,
                        rule.config)

            #
            # If ruleSet are marked that stops at first found
            #
            if r.content:

                # Only store the result of last rule or when rules stops
                if exit_on_found or rule_number == total_rules:

                    if report == "original":
                        value_to_store = previous_data.to_dict
                    elif report == "group":
                        value_to_store = r.content
                    else:
                        value_to_store = previous_data.json_property(report)

                    result.append(PluginReturnedData(
                        **{rule_set.map_to: value_to_store}
                    ))

                if exit_on_found:
                    break

    next_step_results = PluginReturnedData(
        **dict(
            extractor_results=result
        )
    )
    execute_hooks_step(next_step_results, crawler_name)


def execute_hooks_step(previous_data: PluginReturnedData,
                       crawler_name: str):
    crawler = current_config.get_config_crawler(crawler_name)

    for model in crawler.model.hooks:
        # Generated by $generators
        map_concurrently(crawler_name=crawler_name,
                         previous_result=previous_data,
                         model=model)
